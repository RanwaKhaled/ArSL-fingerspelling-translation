{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live feed testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import seaborn \n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy,CategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# import time\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label\n",
    "# from PIL import Image, ImageTk\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of individuals: 31\n",
      "\n",
      "Name of the individuals : \n",
      "\t['Ain', 'Al', 'Alef', 'Beh', 'Dad', 'Dal', 'Feh', 'Ghain', 'Hah', 'Heh', 'Jeem', 'Kaf', 'Khah', 'Laa', 'Lam', 'Meem', 'Noon', 'Qaf', 'Reh', 'Sad', 'Seen', 'Sheen', 'Tah', 'Teh', 'Teh_Marbuta', 'Thal', 'Theh', 'Waw', 'Yeh', 'Zah', 'Zain']\n"
     ]
    }
   ],
   "source": [
    "root_path = \"ArSL dataset/RGB ArSL dataset/\"\n",
    "\n",
    "# Collect all alphapets\n",
    "alphabets = os.listdir(root_path)\n",
    "num_alpha = len(alphabets)\n",
    "\n",
    "print(f\"Total number of individuals: {num_alpha}\\n\")\n",
    "print(f\"Name of the individuals : \\n\\t{alphabets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          label\n",
      "0           Ain\n",
      "1            Al\n",
      "2          Alef\n",
      "3           Beh\n",
      "4           Dad\n",
      "5           Dal\n",
      "6           Feh\n",
      "7         Ghain\n",
      "8           Hah\n",
      "9           Heh\n",
      "10         Jeem\n",
      "11          Kaf\n",
      "12         Khah\n",
      "13          Laa\n",
      "14          Lam\n",
      "15         Meem\n",
      "16         Noon\n",
      "17          Qaf\n",
      "18          Reh\n",
      "19          Sad\n",
      "20         Seen\n",
      "21        Sheen\n",
      "22          Tah\n",
      "23          Teh\n",
      "24  Teh_Marbuta\n",
      "25         Thal\n",
      "26         Theh\n",
      "27          Waw\n",
      "28          Yeh\n",
      "29          Zah\n",
      "30         Zain\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Data_Alph = pd.DataFrame({'label': alphabets})\n",
    "print(Data_Alph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "franco_to_arabic = {'Ain':\"ع\", 'Al':\"ال\", 'Alef':\"ا\", 'Beh':\"ب\", 'Dad':\"ًص\", 'Dal':\"د\", 'Feh':\"ف\", 'Ghain':\"غ\", 'Hah':\"ح\", 'Heh':\"ه\", 'Jeem':\"ج\", 'Kaf':\"ك\", 'Khah':\"ك\", 'Laa':\"لا\", 'Lam':\"ل\", 'Meem':\"م\", 'Noon':\"ن\", 'Qaf':\"ق\", 'Reh':\"ر\", 'Sad':\"ص\", 'Seen':\"س\", 'Sheen':\"ش\", 'Tah':\"ط\", 'Teh':\"ت\", 'Teh_Marbuta':\"ة\", 'Thal':\"ذ\", 'Theh':\"ث\", 'Waw':\"و\", 'Yeh':\"ي\", 'Zah':\"ظ\", 'Zain':\"ز\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ي'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "franco_to_arabic.get(\"Yeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Teh_Marbuta'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(franco_to_arabic)[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"mlp_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_live_landmarks(frame, test_landmarks):\n",
    "    test_normalized_landmarks = []\n",
    "    for landmark_list in test_landmarks:\n",
    "        #print(path)\n",
    "        # print(landmark_list[0])\n",
    "        # for hand_landmarks in landmark_list[0]:\n",
    "        normalized_landmarks = calc_landmark_list(frame, landmark_list[0])\n",
    "        normalized_landmarks = pre_process_landmark(normalized_landmarks)\n",
    "        test_normalized_landmarks.append(normalized_landmarks)\n",
    "    return (test_normalized_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ا\n",
      "ان\n",
      "انا\n",
      "انا ا\n",
      "انا اس\n",
      "انا اسم\n",
      "انا اسمي\n",
      "انا اسمي ف\n",
      "انا اسمي فر\n",
      "انا اسمي فرح\n",
      "انا اسمي فرح ا\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "guide = cv2.imread('guide.png')\n",
    "\n",
    "count_repeated = 0 \n",
    "last_predicted = None\n",
    "space = 0\n",
    "\n",
    "sentence = []\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.1,\n",
    "    min_tracking_confidence=0.3) as hands:\n",
    "  \n",
    "\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "    image = cv2.flip(image,1)\n",
    "    guide = cv2.resize(guide, (guide.shape[1], image.shape[0]))\n",
    "    image = cv2.resize(image, (image.shape[1], guide.shape[0]))\n",
    "    image = np.concatenate((image, guide), axis=1) \n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "      space =0\n",
    "      detected_landmarks = normalize_live_landmarks(image, [results.multi_hand_landmarks])\n",
    "      # print(detected_landmarks)\n",
    "      \n",
    "      harf = model.predict(np.array(detected_landmarks), verbose=0)\n",
    "      harf = [np.argmax(probs) for probs in harf]\n",
    "      \n",
    "      # get the letter from the letter guide using its index\n",
    "      # predicted_letters = [Data_Alph.to_numpy()[letter] for letter in harf]\n",
    "      predicted_letters = Data_Alph.to_numpy()[harf]\n",
    "      if (predicted_letters[0] == last_predicted):\n",
    "          if (count_repeated == 40):\n",
    "            # el mafrod hena baa a2olo y-consider el hard w y-add fl gomla\n",
    "            text = franco_to_arabic.get(predicted_letters[0][0])\n",
    "            sentence.append(text)\n",
    "            print(\"\".join(sentence))\n",
    "\n",
    "\n",
    "            count_repeated = 0\n",
    "          else: \n",
    "            count_repeated+=1\n",
    "      \n",
    "      last_predicted = predicted_letters[0]\n",
    "      txt_place = (10, 10) \n",
    "\n",
    "      text = franco_to_arabic.get(predicted_letters[0][0])\n",
    "      reshaped_text = arabic_reshaper.reshape(text)\n",
    "      bidi_text = get_display(reshaped_text) \n",
    "      fontpath = \"arial.ttf\" \n",
    "      font = ImageFont.truetype(fontpath, 32)\n",
    "      img_pil = Image.fromarray(image)\n",
    "      draw = ImageDraw.Draw(img_pil)\n",
    "      # bbox = draw.textbbox((10,10), bidi_text, font=font)\n",
    "      # draw.rectangle(bbox, fill=(255, 255, 255, 0))\n",
    "      draw.text(txt_place ,bidi_text, font = font , fill = (20,0, 255,0))\n",
    "      image = np.array(img_pil)\n",
    "\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    else:\n",
    "      cv2.putText(image, \"No hand detected yet!!\", (10, 30) ,  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (20,0, 255), 2)\n",
    "      space+=1\n",
    "      # print(space)\n",
    "      if space == 50 and len(sentence)!=0 and sentence[len(sentence)-1] != \" \":\n",
    "        sentence.append(\" \")\n",
    "\n",
    "    str_sentence = \"\".join(sentence)\n",
    "    reshaped_text = arabic_reshaper.reshape(str_sentence)\n",
    "    bidi_text = get_display(reshaped_text) \n",
    "    fontpath = \"arial.ttf\" \n",
    "    font = ImageFont.truetype(fontpath, 32)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    bbox = draw.textbbox((100,420), bidi_text, font=font)\n",
    "    draw.rectangle(bbox, fill=(255, 255, 255, 0))\n",
    "    draw.text((100, 420) ,bidi_text, font = font , fill =(20,0, 0,0))\n",
    "    image = np.array(img_pil)\n",
    "    # guide = cv2.resize(guide, (guide.shape[1], image.shape[0]))\n",
    "    # image = cv2.resize(image,(image.shape[1], guide.shape[0]))\n",
    "    # image = np.concatenate((image, guide), axis=1)\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
